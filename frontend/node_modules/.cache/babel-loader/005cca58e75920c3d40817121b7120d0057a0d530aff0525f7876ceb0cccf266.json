{"ast":null,"code":"// Speech Recognition Setup\nlet recognition;\nif ('SpeechRecognition' in window || 'webkitSpeechRecognition' in window) {\n  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n  recognition = new SpeechRecognition();\n  recognition.continuous = true; // Listen continuously\n  recognition.interimResults = false;\n  recognition.lang = 'en-US';\n} else {\n  console.error('Speech Recognition not supported');\n}\n\n// TTS Setup\nconst speak = (text, onEnd = () => {}) => {\n  const utterance = new SpeechSynthesisUtterance(text);\n  utterance.onend = onEnd;\n  speechSynthesis.speak(utterance);\n};\nconst startListening = onResult => {\n  if (recognition) {\n    recognition.onresult = event => {\n      const command = event.results[event.results.length - 1][0].transcript.toLowerCase().trim();\n      onResult(command);\n    };\n    recognition.start();\n  }\n};\nconst stopListening = () => {\n  if (recognition) recognition.stop();\n};\nexport { speak, startListening, stopListening };","map":{"version":3,"names":["recognition","window","SpeechRecognition","webkitSpeechRecognition","continuous","interimResults","lang","console","error","speak","text","onEnd","utterance","SpeechSynthesisUtterance","onend","speechSynthesis","startListening","onResult","onresult","event","command","results","length","transcript","toLowerCase","trim","start","stopListening","stop"],"sources":["C:/Users/KIIT0001/Desktop/accessible-learning-platform/frontend/src/utils/voiceUtils.js"],"sourcesContent":["// Speech Recognition Setup\r\nlet recognition;\r\nif ('SpeechRecognition' in window || 'webkitSpeechRecognition' in window) {\r\n  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\r\n  recognition = new SpeechRecognition();\r\n  recognition.continuous = true; // Listen continuously\r\n  recognition.interimResults = false;\r\n  recognition.lang = 'en-US';\r\n} else {\r\n  console.error('Speech Recognition not supported');\r\n}\r\n\r\n// TTS Setup\r\nconst speak = (text, onEnd = () => {}) => {\r\n  const utterance = new SpeechSynthesisUtterance(text);\r\n  utterance.onend = onEnd;\r\n  speechSynthesis.speak(utterance);\r\n};\r\n\r\nconst startListening = (onResult) => {\r\n  if (recognition) {\r\n    recognition.onresult = (event) => {\r\n      const command = event.results[event.results.length - 1][0].transcript.toLowerCase().trim();\r\n      onResult(command);\r\n    };\r\n    recognition.start();\r\n  }\r\n};\r\n\r\nconst stopListening = () => {\r\n  if (recognition) recognition.stop();\r\n};\r\n\r\nexport { speak, startListening, stopListening };"],"mappings":"AAAA;AACA,IAAIA,WAAW;AACf,IAAI,mBAAmB,IAAIC,MAAM,IAAI,yBAAyB,IAAIA,MAAM,EAAE;EACxE,MAAMC,iBAAiB,GAAGD,MAAM,CAACC,iBAAiB,IAAID,MAAM,CAACE,uBAAuB;EACpFH,WAAW,GAAG,IAAIE,iBAAiB,CAAC,CAAC;EACrCF,WAAW,CAACI,UAAU,GAAG,IAAI,CAAC,CAAC;EAC/BJ,WAAW,CAACK,cAAc,GAAG,KAAK;EAClCL,WAAW,CAACM,IAAI,GAAG,OAAO;AAC5B,CAAC,MAAM;EACLC,OAAO,CAACC,KAAK,CAAC,kCAAkC,CAAC;AACnD;;AAEA;AACA,MAAMC,KAAK,GAAGA,CAACC,IAAI,EAAEC,KAAK,GAAGA,CAAA,KAAM,CAAC,CAAC,KAAK;EACxC,MAAMC,SAAS,GAAG,IAAIC,wBAAwB,CAACH,IAAI,CAAC;EACpDE,SAAS,CAACE,KAAK,GAAGH,KAAK;EACvBI,eAAe,CAACN,KAAK,CAACG,SAAS,CAAC;AAClC,CAAC;AAED,MAAMI,cAAc,GAAIC,QAAQ,IAAK;EACnC,IAAIjB,WAAW,EAAE;IACfA,WAAW,CAACkB,QAAQ,GAAIC,KAAK,IAAK;MAChC,MAAMC,OAAO,GAAGD,KAAK,CAACE,OAAO,CAACF,KAAK,CAACE,OAAO,CAACC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,CAACC,UAAU,CAACC,WAAW,CAAC,CAAC,CAACC,IAAI,CAAC,CAAC;MAC1FR,QAAQ,CAACG,OAAO,CAAC;IACnB,CAAC;IACDpB,WAAW,CAAC0B,KAAK,CAAC,CAAC;EACrB;AACF,CAAC;AAED,MAAMC,aAAa,GAAGA,CAAA,KAAM;EAC1B,IAAI3B,WAAW,EAAEA,WAAW,CAAC4B,IAAI,CAAC,CAAC;AACrC,CAAC;AAED,SAASnB,KAAK,EAAEO,cAAc,EAAEW,aAAa","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}